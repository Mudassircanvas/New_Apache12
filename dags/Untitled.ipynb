{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d6b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import datetime\n",
    "import aiohttp  #\n",
    "from datetime import date as dt\n",
    "from datetime import time as tm\n",
    "import pytz\n",
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "import nest_asyncio\n",
    "import sys\n",
    "import pickle\n",
    "# import talib\n",
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    \n",
    "    config = {\n",
    "\t\"ticker\": \"AAPL\",\n",
    "    \"start_date\": \"2023-05-01\",\n",
    "    \"end_date\": \"2023-05-08\"\n",
    "    }\n",
    "    \n",
    "    global data_dict\n",
    "    data_dict = {}\n",
    "    #ticker_info = {}\n",
    "\n",
    "    def unix_to_date(dataset, col_name):\n",
    "        dataset[col_name] = pd.to_datetime(dataset[col_name])\n",
    "        dataset[col_name] = dataset[col_name].dt.tz_localize('UTC')\n",
    "        dataset[col_name] = dataset[col_name].dt.tz_convert('US/Eastern')\n",
    "        dataset[col_name] = dataset[col_name].dt.tz_localize(None)\n",
    "        return dataset[col_name]\n",
    "\n",
    "\n",
    "    def daterange(date1, date2):\n",
    "        for n in range(int((date2 - date1).days) + 1):\n",
    "            yield date1 + timedelta(n)\n",
    "\n",
    "\n",
    "    async def get(\n",
    "        session: aiohttp.ClientSession,\n",
    "        date: str,\n",
    "        **kwargs\n",
    "    ) -> dict:\n",
    "        global data_dict\n",
    "        api = f\"https://api.polygon.io/v3/trades/{ticker}?timestamp={date}&apiKey=Ot5XxPIdM4IAsPj6TdlIqHajQFK356JB&limit=50000\"\n",
    "        resp = await session.request('GET', url=api, **kwargs)\n",
    "        data = await resp.json()\n",
    "        data_dict[date] = data\n",
    "        next_url = data.get(\"next_url\", None)\n",
    "        while next_url is not None:\n",
    "            next_url_ = next_url+\"&apiKey=Ot5XxPIdM4IAsPj6TdlIqHajQFK356JB&limit=50000\"\n",
    "            resp = await session.request('GET', url=next_url_, **kwargs)\n",
    "            data = await resp.json()\n",
    "            data_dict[date][\"results\"] += data[\"results\"]\n",
    "            next_url = data.get(\"next_url\", None)\n",
    "\n",
    "\n",
    "    async def main(dates, **kwargs):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for c in dates:\n",
    "                tasks.append(get(session=session, date=c, **kwargs))\n",
    "            responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            return responses\n",
    "\n",
    "  ############################################################################################\n",
    "\n",
    "    print(\"THE CURRENT TICKER IS -> \", config[\"ticker\"])\n",
    "    print(\"FOR START DATE -> \", config[\"start_date\"])\n",
    "    print(\"FOR END DATE -> \", config[\"end_date\"])\n",
    "    ticker = config[\"ticker\"]\n",
    "    start_date = config[\"start_date\"]\n",
    "    end_date = config[\"end_date\"]\n",
    "    start_date_fixed = start_date\n",
    "    end_date_fixed = end_date\n",
    "    path = \"./docker_storage/\"\n",
    "    dir_list = os.listdir(path)\n",
    "    available_tickers = []\n",
    "    for filename in dir_list:\n",
    "        if \"Tick-Data\" in filename:\n",
    "            ticker_name = filename.split('-')\n",
    "            available_tickers.append(ticker_name[0])\n",
    "    print(available_tickers)\n",
    "    \n",
    "    if ticker not in available_tickers:\n",
    "        print(f\"FETCHING DATA FOR {ticker}\")\n",
    "\n",
    "\n",
    "        date_lst = []\n",
    "        while start_date < end_date:\n",
    "            start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            start_date += timedelta(days = 1)\n",
    "            temp_date = start_date\n",
    "            start_date += timedelta(days = 2)\n",
    "            temp_date = temp_date.strftime('%Y-%m-%d')\n",
    "            start_date = start_date.strftime('%Y-%m-%d')\n",
    "            date_lst.append([temp_date, start_date])\n",
    "        #date_lst = pickle.load(open('date_list.pkl', 'rb'))\n",
    "\n",
    "        for start_date, end_date in date_lst:\n",
    "            dates = []\n",
    "            for i in daterange(pd.to_datetime(start_date), pd.to_datetime(end_date)):\n",
    "                dates.append(i.date().strftime(\"%Y-%m-%d\"))\n",
    "            print(dates)\n",
    "            asyncio.run(main(dates))\n",
    "            new_dict = []\n",
    "\n",
    "            for index, i in enumerate(data_dict):\n",
    "                if 'results' not in list(data_dict[i].keys()):\n",
    "                    pass\n",
    "                else:\n",
    "                    new_dict = new_dict + data_dict[i]['results']\n",
    "            df = pd.DataFrame(new_dict)\n",
    "            if len(df) != 0:\n",
    "                if \"participant_timestamp\" not in df.columns:\n",
    "                    df[\"participant_timestamp\"] = df[\"sip_timestamp\"]\n",
    "                df['participant_timestamp'] = unix_to_date(df, \"participant_timestamp\")\n",
    "                #df['sip_timestamp'] = unix_to_date(df, \"sip_timestamp\")\n",
    "                df = df.sort_values(by=\"participant_timestamp\")\n",
    "                df = df.set_index(\"participant_timestamp\")\n",
    "                df = df[[\"price\", \"size\"]]\n",
    "                df = df.reset_index()\n",
    "                df[\"participant_timestamp\"] = df[\"participant_timestamp\"]\n",
    "                ftr_files = glob.glob(os.path.join('./docker_storage/', f\"{ticker}-|{start_date_fixed}_{end_date_fixed}|-Tick-Data.ftr\"))\n",
    "\n",
    "                if len(ftr_files) == 0:\n",
    "                    df.to_feather(f\"./docker_storage/{ticker}-|{start_date_fixed}_{end_date_fixed}|-Tick-Data.ftr\")\n",
    "                else:\n",
    "                    if 'df3' not in locals():\n",
    "                        df2 = pd.read_feather(f\"./docker_storage/{ticker}-|{start_date_fixed}_{end_date_fixed}|-Tick-Data.ftr\")\n",
    "                    else:\n",
    "                        df2 = df3\n",
    "                    df3 = df2.append(df)\n",
    "                    df3 = df3.reset_index(drop = True)\n",
    "                    df3.to_feather(f\"./docker_storage/{ticker}-|{start_date_fixed}_{end_date_fixed}|-Tick-Data.ftr\")\n",
    "                    del df2  # memory flush\n",
    "                    #del df3\n",
    "                del df\n",
    "            else:\n",
    "                print(f\"NO DATA FOR THIS DURATION -> {start_date}-{end_date}\")\n",
    "                \n",
    "            del data_dict\n",
    "            data_dict = {}\n",
    "\n",
    "        path_loc = f\"./docker_storage/{ticker}-|{start_date_fixed}_{end_date_fixed}|-Tick-Data.ftr\"\n",
    "        ticker_info = {\"name\" : ticker, \"path\" : path_loc, \"start-date\" : start_date_fixed, \"end-date\" : end_date_fixed}\n",
    "        return ticker_info\n",
    "    else:\n",
    "        print(f\"DATA FOR {ticker} ALREADY EXISTS\")\n",
    "        for filename in dir_list:\n",
    "            if \"Tick-Data\" in filename:\n",
    "                ticker_name = filename.split('-')[0]\n",
    "                print(\"ticker_name\", ticker_name, \"ticker\", ticker)\n",
    "                if ticker_name == ticker:\n",
    "                    data_name = filename\n",
    "                    dates = filename.split('|')[1]\n",
    "                    dates = dates.replace('_', '-')\n",
    "                    dates = dates.split('-')\n",
    "                    start_date = dates[0:3]\n",
    "                    start_date = '-'.join(start_date)\n",
    "                    end_date = dates[3:6]\n",
    "                    end_date = '-'.join(end_date)\n",
    "                    break\n",
    "        path_loc = f\"./docker_storage/{data_name}\"\n",
    "        ticker_info = {\"name\" : ticker, \"path\" : path_loc, \"start-date\" : start_date, \"end-date\" : end_date}\n",
    "        return ticker_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d14a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE CURRENT TICKER IS ->  AAPL\n",
      "FOR START DATE ->  2023-05-01\n",
      "FOR END DATE ->  2023-05-08\n",
      "[]\n",
      "FETCHING DATA FOR AAPL\n",
      "['2023-05-02', '2023-05-03', '2023-05-04']\n",
      "['2023-05-05', '2023-05-06', '2023-05-07']\n",
      "['2023-05-08', '2023-05-09', '2023-05-10']\n"
     ]
    }
   ],
   "source": [
    "ticker_info = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7d5941",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlfinlab.data_structures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#import seaborn as sns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlfinlab\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imbalance_data_structures, standard_data_structures\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdateutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelativedelta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relativedelta, MO\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlfinlab.data_structures'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from mlfinlab.data_structures import imbalance_data_structures, standard_data_structures\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta, MO\n",
    "import gc\n",
    "import talib\n",
    "\n",
    "\n",
    "def step_one(ticker_info):\n",
    "    tic = ticker_info[\"name\"]\n",
    "    path = ticker_info[\"path\"]\n",
    "    start_date = ticker_info[\"start-date\"]\n",
    "    end_date = ticker_info[\"end-date\"]\n",
    "\n",
    "    if tic not in ['BTC', 'ETH']:\n",
    "        print(tic)\n",
    "        raw_merged_feather = pd.read_feather(path)\n",
    "        raw_merged_feather.reset_index(inplace=True, drop=True)\n",
    "        #temp=raw_merged_feather[raw_merged_feather.participant_timestamp<'2017-05-01'].copy()               #UNCOMMENT WHEN ON SERVER!\n",
    "        temp = raw_merged_feather.copy()        \n",
    "        temp=temp[['participant_timestamp', 'price', 'size']]\n",
    "        temp.dropna(inplace=True)\n",
    "        temp['dv']=temp['price']*temp['size']\n",
    "        volthresh=temp.groupby([temp['participant_timestamp'].dt.date]).sum().dv.describe(percentiles=[0.6])['60%']\n",
    "#-------------------------------------------------------------------------\n",
    "        temp=raw_merged_feather[raw_merged_feather.participant_timestamp>'2017-01-01'].copy()        \n",
    "        temp['sma_price']=talib.SMA(temp['price'], 10)\n",
    "        temp['sma_price_pct']=abs(((temp['price']-temp['sma_price'])/temp['sma_price'])*100)\n",
    "        temp=temp[temp['sma_price_pct']<7.5].reset_index(drop=True)\n",
    "        temp=temp[['participant_timestamp', 'price', 'size']]\n",
    "        temp.dropna(inplace=True)\n",
    "        temp=temp[temp.participant_timestamp>'2017-05-01'].copy()\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "        temp.to_feather(f'./docker_storage/Tick_Data/AdjustedData/2k17OnAndImputed/{tic}-|{start_date}_{end_date}|-Tick-Data.ftr')\n",
    "        \n",
    "        gc.collect()\n",
    "        for barsperday in [10]:\n",
    "            t=(volthresh/barsperday)\n",
    "            dolDF=standard_data_structures.get_dollar_bars(temp, threshold=t, verbose=False, batch_size=100000)\n",
    "            dolDF.to_feather(f'./docker_storage/Tick_Data/Const_Resampled_2017/{tic}_dolDF_const_{barsperday}_BarsPerDay.ftr')\n",
    "            gc.collect()\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(tic)\n",
    "        raw_merged_feather = pd.read_feather(path)\n",
    "        raw_merged_feather.reset_index(inplace=True, drop=True)\n",
    "        temp=raw_merged_feather[raw_merged_feather.participant_timestamp<'2018-05-01'].copy()\n",
    "        temp=temp[['participant_timestamp', 'price', 'size']]\n",
    "        temp.dropna(inplace=True)\n",
    "        temp['dv']=temp['price']*temp['size']\n",
    "        volthresh=temp.groupby([temp['participant_timestamp'].dt.date]).sum().dv.describe(percentiles=[0.6])['60%']\n",
    "#-------------------------------------------------------------------------\n",
    "        temp=raw_merged_feather[raw_merged_feather.participant_timestamp>'2018-01-01'].copy()        \n",
    "        temp['sma_price']=talib.SMA(temp['price'], 10)\n",
    "        temp['sma_price_pct']=abs(((temp['price']-temp['sma_price'])/temp['sma_price'])*100)\n",
    "        temp=temp[temp['sma_price_pct']<7.5].reset_index(drop=True)\n",
    "        temp=temp[['participant_timestamp', 'price', 'size']]\n",
    "        temp.dropna(inplace=True)\n",
    "        temp=temp[temp.participant_timestamp>'2018-05-01'].copy()\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "        temp.to_feather(f'./docker_storage/Tick_Data/AdjustedData/2k17OnAndImputed/{tic}-{start_date}_{end_date}-Tick-Data.ftr')\n",
    "\n",
    "        gc.collect()\n",
    "        for barsperday in [10]:\n",
    "            t=(volthresh/(barsperday*3.69))\n",
    "            dolDF=standard_data_structures.get_dollar_bars(temp, threshold=t, verbose=False, batch_size=100000)\n",
    "            dolDF.to_feather(f'/docker_storage/Tick_Data/Const_Resampled_2017/{tic}_dolDF_const_{barsperday}_BarsPerDay.ftr')\n",
    "            gc.collect()\n",
    "        gc.collect()\n",
    "    ticker_info['resample-path'] = f\"./docker_storage/Tick_Data/Const_Resampled_2017/{tic}_dolDF_const_{barsperday}_BarsPerDay.ftr\"\n",
    "    ticker_info[\"impute-path\"] = f\"./docker_storage/Tick_Data/AdjustedData/2k17OnAndImputed/{tic}-|{start_date}_{end_date}|-Tick-Data.ftr\"\n",
    "    return ticker_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021875d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
